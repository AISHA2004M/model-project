{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fea18f-1ff8-439f-9057-0bc43a768824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_and_run_xgb.py\n",
    "\"\"\"\n",
    "Phishing detection — training + interactive test using XGBoost.\n",
    "Reads CSV at DATA_FILE (default: your provided path).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import html\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import joblib\n",
    "\n",
    "# Use XGBoost if installed; fallback to sklearn RandomForest if not.\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "# ================ CONFIG ================\n",
    "DATA_FILE = '/Users/yahyamohnd/Downloads/URL_final.csv'   # <-- عدل هنا إن لزم\n",
    "OUT_MODEL = 'xgb_phish_model.pkl'\n",
    "OUT_VECT_WORD = 'tfidf_word.pkl'\n",
    "OUT_VECT_CHAR = 'tfidf_char.pkl'\n",
    "OUT_SCALER = 'url_scaler.pkl'\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "# ========================================\n",
    "\n",
    "if not os.path.exists(DATA_FILE):\n",
    "    raise FileNotFoundError(f\"Data file not found: {DATA_FILE}\")\n",
    "\n",
    "print(\"Loading data:\", DATA_FILE)\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "print(\"Raw columns:\", list(df.columns))\n",
    "\n",
    "# If there's no 'text' column, try 'URL' or first string column\n",
    "if 'text' not in df.columns:\n",
    "    if 'URL' in df.columns:\n",
    "        df['text'] = df['URL'].astype(str)\n",
    "    else:\n",
    "        text_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "        if len(text_cols) > 0:\n",
    "            df['text'] = df[text_cols[0]].astype(str)\n",
    "        else:\n",
    "            raise ValueError(\"No text-like column found in CSV (expecting 'text' or 'URL' or other string column).\")\n",
    "\n",
    "# Ensure label exists and is integer 0/1\n",
    "if 'label' not in df.columns:\n",
    "    raise ValueError(\"CSV must contain a 'label' column with 0 (legit) / 1 (phish).\")\n",
    "\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# ---------- text cleaning ----------\n",
    "def clean_text_basic(s):\n",
    "    if pd.isna(s): return ''\n",
    "    s = str(s)\n",
    "    s = html.unescape(s)\n",
    "    s = re.sub(r'<[^>]+>', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "# Advanced normalization for text (keeps <URL> token)\n",
    "def clean_text_advanced(s):\n",
    "    s = clean_text_basic(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'http\\S+|www\\.\\S+', '<URL>', s)\n",
    "    s = re.sub(r'\\d+', '<NUM>', s)\n",
    "    s = re.sub(r'[^\\w\\s<>]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text_advanced)\n",
    "\n",
    "# ---------- URL feature extraction ----------\n",
    "def extract_url_features(url_text):\n",
    "    # if text does not look like URL, return zeros (we still produce features for consistency)\n",
    "    # We'll treat strings with '<url>' or starting with http/www as URLs\n",
    "    s = str(url_text)\n",
    "    is_url = ('<url>' in s) or s.startswith('http') or s.startswith('www') or '://' in s\n",
    "    if not is_url:\n",
    "        # zero features\n",
    "        return {k:0 for k in url_feature_cols}\n",
    "\n",
    "    p = urlparse(s if '://' in s else 'http://'+s)\n",
    "    scheme = p.scheme\n",
    "    netloc = p.netloc\n",
    "    path = p.path or ''\n",
    "    query = p.query or ''\n",
    "    full = s if '://' in s else 'http://'+s\n",
    "\n",
    "    url_length = len(full)\n",
    "    # simplistic IP check: host is 4 numeric parts\n",
    "    parts = netloc.split('.')\n",
    "    has_ip_address = 1 if len(parts)==4 and all(pt.isdigit() for pt in parts) else 0\n",
    "    dot_count = netloc.count('.') + path.count('.')\n",
    "    https_flag = 1 if scheme == 'https' else 0\n",
    "    uniq_chars = len(set(full))\n",
    "    url_entropy = round((uniq_chars / (len(full)+1)) * 10, 3)\n",
    "    token_count = max(1, len([t for t in (path + ('?'+query if query else '')).split('/') if t]))\n",
    "    subdomain_count = max(0, netloc.count('.') - 1)\n",
    "    query_param_count = 1 if query else 0\n",
    "    tld = netloc.split('.')[-1] if '.' in netloc else ''\n",
    "    tld_length = len(tld)\n",
    "    path_length = len(path)\n",
    "    has_hyphen_in_domain = 1 if '-' in netloc else 0\n",
    "    number_of_digits = sum(c.isdigit() for c in full)\n",
    "    tld_pop_map = {'com':1000,'org':300,'net':200,'io':100,'co':150,'info':50}\n",
    "    tld_popularity = tld_pop_map.get(tld, 10)\n",
    "    suspicious_file_extension = 1 if any(full.endswith(ext) for ext in ('.exe','.zip','.php','.asp')) else 0\n",
    "    domain_name_length = len(netloc)\n",
    "    percentage_numeric_chars = round((number_of_digits / (len(full)+1)) * 100, 3)\n",
    "\n",
    "    return {\n",
    "        \"url_length\": url_length,\n",
    "        \"has_ip_address\": has_ip_address,\n",
    "        \"dot_count\": dot_count,\n",
    "        \"https_flag\": https_flag,\n",
    "        \"url_entropy\": url_entropy,\n",
    "        \"token_count\": token_count,\n",
    "        \"subdomain_count\": subdomain_count,\n",
    "        \"query_param_count\": query_param_count,\n",
    "        \"tld_length\": tld_length,\n",
    "        \"path_length\": path_length,\n",
    "        \"has_hyphen_in_domain\": has_hyphen_in_domain,\n",
    "        \"number_of_digits\": number_of_digits,\n",
    "        \"tld_popularity\": tld_popularity,\n",
    "        \"suspicious_file_extension\": suspicious_file_extension,\n",
    "        \"domain_name_length\": domain_name_length,\n",
    "        \"percentage_numeric_chars\": percentage_numeric_chars\n",
    "    }\n",
    "\n",
    "# Feature column names\n",
    "url_feature_cols = [\n",
    "    'url_length','has_ip_address','dot_count','https_flag','url_entropy','token_count',\n",
    "    'subdomain_count','query_param_count','tld_length','path_length','has_hyphen_in_domain',\n",
    "    'number_of_digits','tld_popularity','suspicious_file_extension','domain_name_length','percentage_numeric_chars'\n",
    "]\n",
    "\n",
    "# If CSV already has these columns, use them; otherwise compute\n",
    "existing_url_features = [c for c in url_feature_cols if c in df.columns]\n",
    "missing_features = [c for c in url_feature_cols if c not in df.columns]\n",
    "if missing_features:\n",
    "    print(\"Computing missing URL features:\", missing_features)\n",
    "    # compute row-wise (vectorized apply)\n",
    "    feats_df = df['text'].apply(lambda t: pd.Series(extract_url_features(t)))\n",
    "    # append only missing\n",
    "    for c in missing_features:\n",
    "        df[c] = feats_df[c].values\n",
    "    existing_url_features = url_feature_cols\n",
    "\n",
    "# Build X (TF-IDF + URL features)\n",
    "# Word-level TF-IDF + char-level TF-IDF (use both)\n",
    "vect_word = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=4000)\n",
    "vect_char = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,5), max_features=3000)\n",
    "\n",
    "Xw = vect_word.fit_transform(df['text'])\n",
    "Xc = vect_char.fit_transform(df['text'])\n",
    "\n",
    "# URL dense features -> standardize\n",
    "X_url = df[existing_url_features].fillna(0).astype(float).values\n",
    "scaler = StandardScaler()\n",
    "X_url_scaled = scaler.fit_transform(X_url)\n",
    "\n",
    "# Final X\n",
    "X = hstack([Xw, Xc, csr_matrix(X_url_scaled)])\n",
    "y = df['label'].astype(int).values\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Labels distribution:\", np.bincount(y))\n",
    "\n",
    "# Train/test split (stratify)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE,\n",
    "                                                    random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# If using XGBoost, compute scale_pos_weight to counter imbalance\n",
    "pos = (y_train==1).sum()\n",
    "neg = (y_train==0).sum()\n",
    "scale_pos_weight = neg/pos if pos>0 else 1.0\n",
    "print(\"Train pos/neg:\", pos, neg, \"scale_pos_weight:\", round(scale_pos_weight,3))\n",
    "\n",
    "# Choose model\n",
    "if XGB_AVAILABLE:\n",
    "    print(\"Training XGBoost classifier...\")\n",
    "    clf = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "else:\n",
    "    print(\"XGBoost not available — falling back to RandomForest\")\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators=300, class_weight='balanced', n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)[:,1] if hasattr(clf, \"predict_proba\") else None\n",
    "\n",
    "print(\"\\n=== Evaluation on test set ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "if y_prob is not None:\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Save models/tools\n",
    "joblib.dump(clf, OUT_MODEL)\n",
    "joblib.dump(vect_word, OUT_VECT_WORD)\n",
    "joblib.dump(vect_char, OUT_VECT_CHAR)\n",
    "joblib.dump(scaler, OUT_SCALER)\n",
    "print(\"Saved model & vectorizers:\", OUT_MODEL, OUT_VECT_WORD, OUT_VECT_CHAR, OUT_SCALER)\n",
    "\n",
    "# ---------------- interactive classifier ----------------\n",
    "def classify_input(text, threshold=0.5):\n",
    "    txt_clean = clean_text_advanced(text)\n",
    "    v_w = vect_word.transform([txt_clean])\n",
    "    v_c = vect_char.transform([txt_clean])\n",
    "    # build url features from input text\n",
    "    feats = extract_url_features(text)\n",
    "    vals = np.array([feats[c] for c in existing_url_features]).reshape(1,-1)\n",
    "    vals_scaled = scaler.transform(vals)\n",
    "    X_in = hstack([v_w, v_c, csr_matrix(vals_scaled)])\n",
    "    prob = clf.predict_proba(X_in)[:,1][0] if hasattr(clf, \"predict_proba\") else clf.predict(X_in)[0]\n",
    "    pred = \"Phishing\" if prob >= threshold else \"Legit\"\n",
    "    return pred, prob\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nInteractive mode — enter text or URL (type 'exit' to quit).\")\n",
    "    while True:\n",
    "        try:\n",
    "            s = input(\"Enter text or URL: \").strip()\n",
    "        except (KeyboardInterrupt, EOFError):\n",
    "            print(\"\\nExiting.\")\n",
    "            break\n",
    "        if s.lower() == 'exit':\n",
    "            print(\"Bye.\")\n",
    "            break\n",
    "        pred, prob = classify_input(s, threshold=0.5)\n",
    "        print(f\"Prediction: {pred} | Prob(phish) = {prob:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
