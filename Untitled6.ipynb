{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af6508-25bf-4ce4-8dca-388b830039f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "ROC AUC: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     20000\n",
      "           1       1.00      1.00      1.00     20000\n",
      "\n",
      "    accuracy                           1.00     40000\n",
      "   macro avg       1.00      1.00      1.00     40000\n",
      "weighted avg       1.00      1.00      1.00     40000\n",
      "\n",
      "Confusion matrix:\n",
      " [[20000     0]\n",
      " [    0 20000]]\n",
      "\n",
      "Interactive mode â€” enter text or URL (type 'exit'):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  https://www.amazon.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Legit | Prob(phish) = 0.000\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  http://amazon.verify-user.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Phishing | Prob(phish) = 1.000\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  Update your PayPal account immediately to avoid suspension: http://paypal-security-update.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Legit | Prob(phish) = 0.300\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:   \"You've won a $1000 gift card! Click to claim: http://free-giftcard-win.com\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Legit | Prob(phish) = 0.220\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:          \"Alert: Unusual activity detected on your Gmail account. Verify: http://gmail-security-check.com\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Phishing | Prob(phish) = 0.687\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:          \"Your Amazon account has been locked. Verify here: http://amazon.verify-user.com\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Phishing | Prob(phish) = 0.520\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:          \"Update your PayPal account immediately to avoid suspension: http://paypal-security-update.com\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Legit | Prob(phish) = 0.300\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  http://paypal-security-update.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Phishing | Prob(phish) = 1.000\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  https://www.nytimes.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Legit | Prob(phish) = 0.000\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  http://netflix.account-update.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Phishing | Prob(phish) = 1.000\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  Update your PayPal account immediately to avoid suspension: http://paypal-security-update.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Legit | Prob(phish) = 0.300\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  You've won a $1000 gift card! Click to claim: http://free-giftcard-win.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Legit | Prob(phish) = 0.217\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  http://gmail-security-check.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Phishing | Prob(phish) = 1.000\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  http://yanya-hellp.ned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Phishing | Prob(phish) = 0.503\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  https://dz.iq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Legit | Prob(phish) = 0.003\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  http://yaya-lasmdf-dfbjnad/.das\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Phishing | Prob(phish) = 0.743\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  https://hell0.cds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Legit | Prob(phish) = 0.187\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  https://dasda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Legit | Prob(phish) = 0.020\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  https\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Legit | Prob(phish) = 0.260\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  https://alskndlasndl.asdaksn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Legit | Prob(phish) = 0.263\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  http://alskndlasndl.asdaksn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Legit | Prob(phish) = 0.470\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  https://alskndlasndl-sd.sad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Phishing | Prob(phish) = 0.620\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  https://hello-yahya.cpm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Legit | Prob(phish) = 0.327\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text/URL:  https://hello-yahya.cpm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Legit | Prob(phish) = 0.327\n"
     ]
    }
   ],
   "source": [
    "import os, html, re, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DATA_FILE = '/Users/yahyamohnd/Downloads/Phishing_dataset_full_large.csv'\n",
    "OUT_MODEL = 'xgb_phish_model.pkl'\n",
    "OUT_VECT_WORD = 'tfidf_word.pkl'\n",
    "OUT_VECT_CHAR = 'tfidf_char.pkl'\n",
    "OUT_SCALER = 'url_scaler.pkl'\n",
    "TEST_SIZE = 0.4\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --------------- Load Data ----------------\n",
    "if not os.path.exists(DATA_FILE):\n",
    "    raise FileNotFoundError(f\"Data file not found: {DATA_FILE}\")\n",
    "\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "if 'text' not in df.columns:\n",
    "    if 'URL' in df.columns:\n",
    "        df['text'] = df['URL'].astype(str)\n",
    "    else:\n",
    "        text_cols = [c for c in df.columns if df[c].dtype==object]\n",
    "        if len(text_cols)>0:\n",
    "            df['text'] = df[text_cols[0]].astype(str)\n",
    "        else:\n",
    "            raise ValueError(\"No text column found\")\n",
    "\n",
    "if 'label' not in df.columns:\n",
    "    raise ValueError(\"CSV must contain a 'label' column with 0/1\")\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# --------------- Text Cleaning ----------------\n",
    "def clean_text(s):\n",
    "    if pd.isna(s): return ''\n",
    "    s = html.unescape(str(s)).lower()\n",
    "    s = re.sub(r'<[^>]+>', ' ', s)\n",
    "    s = re.sub(r'http\\S+|www\\.\\S+', '<URL>', s)\n",
    "    s = re.sub(r'\\d+', '<NUM>', s)\n",
    "    s = re.sub(r'[^\\w\\s<>]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# --------------- URL Features ----------------\n",
    "url_feature_cols = [\n",
    "    'url_length','has_ip_address','dot_count','https_flag','url_entropy','token_count',\n",
    "    'subdomain_count','query_param_count','tld_length','path_length','has_hyphen_in_domain',\n",
    "    'number_of_digits','tld_popularity','suspicious_file_extension','domain_name_length','percentage_numeric_chars'\n",
    "]\n",
    "\n",
    "def extract_url_features(s):\n",
    "    is_url = '<url>' in s or s.startswith('http') or s.startswith('www') or '://' in s\n",
    "    if not is_url:\n",
    "        return {k:0 for k in url_feature_cols}\n",
    "    p = urlparse(s if '://' in s else 'http://'+s)\n",
    "    full = s if '://' in s else 'http://'+s\n",
    "    netloc = p.netloc; path = p.path or ''; query = p.query or ''\n",
    "    url_length = len(full)\n",
    "    parts = netloc.split('.')\n",
    "    has_ip_address = 1 if len(parts)==4 and all(pt.isdigit() for pt in parts) else 0\n",
    "    dot_count = netloc.count('.') + path.count('.')\n",
    "    https_flag = 1 if p.scheme=='https' else 0\n",
    "    uniq_chars = len(set(full))\n",
    "    url_entropy = round((uniq_chars/(len(full)+1))*10,3)\n",
    "    token_count = max(1,len([t for t in (path+('?' + query if query else '')).split('/') if t]))\n",
    "    subdomain_count = max(0, netloc.count('.')-1)\n",
    "    query_param_count = 1 if query else 0\n",
    "    tld = netloc.split('.')[-1] if '.' in netloc else ''\n",
    "    tld_length = len(tld)\n",
    "    path_length = len(path)\n",
    "    has_hyphen_in_domain = 1 if '-' in netloc else 0\n",
    "    number_of_digits = sum(c.isdigit() for c in full)\n",
    "    tld_pop_map = {'com':1000,'org':300,'net':200,'io':100,'co':150,'info':50}\n",
    "    tld_popularity = tld_pop_map.get(tld,10)\n",
    "    suspicious_file_extension = 1 if any(full.endswith(ext) for ext in ['.exe','.zip','.php','.asp']) else 0\n",
    "    domain_name_length = len(netloc)\n",
    "    percentage_numeric_chars = round((number_of_digits/(len(full)+1))*100,3)\n",
    "    return {\n",
    "        \"url_length\": url_length, \"has_ip_address\": has_ip_address, \"dot_count\": dot_count,\n",
    "        \"https_flag\": https_flag, \"url_entropy\": url_entropy, \"token_count\": token_count,\n",
    "        \"subdomain_count\": subdomain_count, \"query_param_count\": query_param_count,\n",
    "        \"tld_length\": tld_length, \"path_length\": path_length, \"has_hyphen_in_domain\": has_hyphen_in_domain,\n",
    "        \"number_of_digits\": number_of_digits, \"tld_popularity\": tld_popularity,\n",
    "        \"suspicious_file_extension\": suspicious_file_extension, \"domain_name_length\": domain_name_length,\n",
    "        \"percentage_numeric_chars\": percentage_numeric_chars\n",
    "    }\n",
    "\n",
    "# Compute missing URL features\n",
    "missing_features = [c for c in url_feature_cols if c not in df.columns]\n",
    "if missing_features:\n",
    "    feats_df = df['text'].apply(lambda t: pd.Series(extract_url_features(t)))\n",
    "    for c in missing_features:\n",
    "        df[c] = feats_df[c].values\n",
    "existing_url_features = url_feature_cols\n",
    "\n",
    "# ---------------- Feature Matrix ----------------\n",
    "vect_word = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=4000)\n",
    "vect_char = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,5), max_features=3000)\n",
    "\n",
    "Xw = vect_word.fit_transform(df['text'])\n",
    "Xc = vect_char.fit_transform(df['text'])\n",
    "X_url_scaled = StandardScaler().fit_transform(df[existing_url_features].values)\n",
    "X = hstack([Xw, Xc, csr_matrix(X_url_scaled)])\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE,\n",
    "                                                    stratify=y, random_state=RANDOM_STATE)\n",
    "\n",
    "# ---------------- Train ----------------\n",
    "pos = (y_train==1).sum(); neg = (y_train==0).sum()\n",
    "scale_pos_weight = neg/pos if pos>0 else 1.0\n",
    "if XGB_AVAILABLE:\n",
    "    clf = XGBClassifier(n_estimators=300,max_depth=6,learning_rate=0.1,\n",
    "                        use_label_encoder=False,eval_metric='logloss',\n",
    "                        scale_pos_weight=scale_pos_weight,random_state=RANDOM_STATE,n_jobs=-1)\n",
    "else:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators=300,class_weight='balanced',random_state=RANDOM_STATE,n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ---------------- Evaluation ----------------\n",
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)[:,1] if hasattr(clf,\"predict_proba\") else None\n",
    "print(\"Accuracy:\", accuracy_score(y_test,y_pred))\n",
    "if y_prob is not None:\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test,y_prob))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test,y_pred))\n",
    "\n",
    "# ---------------- Save ----------------\n",
    "joblib.dump(clf, OUT_MODEL)\n",
    "joblib.dump(vect_word, OUT_VECT_WORD)\n",
    "joblib.dump(vect_char, OUT_VECT_CHAR)\n",
    "joblib.dump(StandardScaler().fit(df[existing_url_features].values), OUT_SCALER)\n",
    "\n",
    "# ---------------- Interactive ----------------\n",
    "def classify_input(text, threshold=0.5):\n",
    "    txt_clean = clean_text(text)\n",
    "    v_w = vect_word.transform([txt_clean])\n",
    "    v_c = vect_char.transform([txt_clean])\n",
    "    vals = np.array([extract_url_features(text)[c] for c in existing_url_features]).reshape(1,-1)\n",
    "    vals_scaled = StandardScaler().fit(df[existing_url_features].values).transform(vals)\n",
    "    X_in = hstack([v_w, v_c, csr_matrix(vals_scaled)])\n",
    "    prob = clf.predict_proba(X_in)[:,1][0] if hasattr(clf,\"predict_proba\") else clf.predict(X_in)[0]\n",
    "    pred = \"Phishing\" if prob>=threshold else \"Legit\"\n",
    "    return pred, prob\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    print(\"\\nInteractive mode â€” enter text or URL (type 'exit'):\")\n",
    "    while True:\n",
    "        s = input(\"Enter text/URL: \").strip()\n",
    "        if s.lower()=='exit': break\n",
    "        pred, prob = classify_input(s)\n",
    "        print(f\"Prediction: {pred} | Prob(phish) = {prob:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
